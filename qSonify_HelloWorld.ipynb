{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qSonify Hello World: Hearing Algorithms to Aid Algorithmic Abstraction\n",
    "#### https://github.com/jiosue/qSonify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how to use qSonify to recognize patterns in quantum algorithms in order to help us understand their decomposition and/or abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qSonify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, the algorithms are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1 = [\"h(0)\", \"cx(0, 1)\", \"cx(0, 2)\", \"cx(0, 3)\", \"cx(0, 4)\"]\n",
    "alg2 = ['h(0)', 'rz(pi, 2)', 'cx(2, 3)', 'rx(pi/2, 2)', 'cx(0, 3)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to choose a mapping from output to midi file. Many mappings require us to choose a set of notes to use, let's define ours here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = (\"c4\", \"d4\", \"e4\", \"f4\", \"g4\", \"a4\", \"b4\", \"c5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our algorithms, we'll use IBM's simulator. If you want to run on actual hardware, note:\n",
    "\n",
    "- You must have your IBM account stored locally. You only need to do this once, via `qiskit.IBMQ.store_account(APItoken)`\n",
    "- If you run your algorithm with markovian sampling, you will sit in the queue many times, because you need to wait for the previous output in order to run the next circuit (see the discussion in the cells below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = qSonify.simulator\n",
    "# To run on the actual quantum computer, uncomment one of the following lines.\n",
    "# backend = \"ibmqx4\"\n",
    "# backend = \"ibmqx5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define/choose our mapping. We can either define our own mapping or choose one of the one's already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping = qSonify.fermionic(notes=notes)\n",
    "#mapping = qSonify.scale(notes=notes)\n",
    "#mapping = qSonify.grandpiano(low_notes=(\"c3\", \"d3\", \"e3\"), high_notes=notes)\n",
    "#mapping = qSonify.stringquartet()\n",
    "#mapping = qSonify.frequencymapping(low_freq=300, base=4)\n",
    "\n",
    "# user_defined mapping must be of this form.\n",
    "# as an examle, this particular user fefined mapping is\n",
    "# the same as qSonify.frequencymapping(low_freq=300, base=4).\n",
    "# To see more details of how to add more tracks, etc, see the\n",
    "# `maps` folder.\n",
    "def mapping(res, name, tempo):\n",
    "    \"\"\"\n",
    "    res: list of strings, outputs of the quantum computer run, ie\n",
    "         ['10010', '01101', ...]\n",
    "    name: str, name of the song.\n",
    "    tempo: int, tempo of the song.\n",
    "    return: qSonify.Song object.\n",
    "    \"\"\"\n",
    "    s = qSonify.Song(name=name, tempo=tempo, num_tracks=1)\n",
    "    for x in res:\n",
    "        note = qSonify.freq_to_note(int(x, base=4) + 300)\n",
    "        s.addNote(note, duration=.5) # eigth notes\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it! We use the function with the following keyword arguments:\n",
    "\n",
    "    qSonify.alg_to_song(algorithm, num_qubits, num_samples, backend, markovian, mapping, name, tempo):\n",
    "        algorithm: algorithm (list of strings), NOT a list of algorithms, \n",
    "                   each string is a gate in GATE_ARGUMENTS.keys() with whatever \n",
    "                   arguments required to define the gate.\n",
    "        num_qubits: int, number of qubits to run each algorithm on. Not required.\n",
    "        num_samples: int, number of samples to take from the quantum computer,\n",
    "                          for most mappings this is equal to the number of beats.\n",
    "        backend: str, IBM backend to run the algorithm on.\n",
    "        markovian: bool, whether to sample using qc.sample or qc.markovian_sample.\n",
    "        mapping: function, which mapping from output to sound to use.\n",
    "        name: str, name of song.\n",
    "        tempo: int, tempo of song.\n",
    "        returns: Song object\n",
    "\n",
    "Markovian sampling is inputting the output of the previous run in as the input to the next run. Note that when running on actual IBM hardware, this will make you sit in the queue `num_samples` times! However, using markovian sampling usually yields better results for hearing the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(num_samples=40, backend=backend, markovian=True, mapping=mapping, tempo=150)\n",
    "\n",
    "s1 = qSonify.alg_to_song(alg1, name=\"HelloWorld_alg1\", **kwargs)\n",
    "s2 = qSonify.alg_to_song(alg2, name=\"HelloWorld_alg2\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to them! *Note the play function only works on Windows right now, because it just calls the system to open the midi file, which Windows by default opens in Windows Media Player. To play the files on your computer if it is non Windows, go to the folder output/ and open the saved .mid file however you want.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can still \"hear\" these algorithms when they are embedded within another circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1_emb = [\"h(4)\", \"rz(pi/8, 2)\"] + alg1 + [\"h(1)\"]\n",
    "alg2_emb = [\"h(2)\", \"rz(pi/4, 0)\"] + alg2 + [\"h(0)\"]\n",
    "s1_emb = qSonify.alg_to_song(alg1_emb, name=\"HelloWorld_alg1_emb\", **kwargs)\n",
    "s2_emb = qSonify.alg_to_song(alg2_emb, name=\"HelloWorld_alg2_emb\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_emb.play() # compare to s1.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_emb.play() # compare to s2.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you hear the similarities? Do you hear the original algorithm embedded within the new one? Now let's try one more thing. Let see if we can hear both algorithm1 and algorithm2 when they are performed in the same algorithm and embedded within other gate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg12_emb = alg1_emb + alg2_emb + alg2_emb + alg1_emb\n",
    "s12_emb = qSonify.alg_to_song(alg12_emb, name=\"HelloWorld_alg12_emb\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to it. Try and compare `s12_emb` to `s1`, `s2`, `s1_emb`, and `s2_emb`, and try to see if you can hear the original `alg1` and `alg2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12_emb.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So did you hear it? Sonifying quantum algorithms in this way could help researchers \"decompile\" algorithms that are represented in terms of elementary gate operations that are generally hard to abstract from. Try testing some different mappings or defining your own! All the midi file outputs from `s.play()` or `s.view()` or `s.writeFile()` are saved in the \"output\" folder in this same directory. By the way, the `s.view()` functionality is computer specific, and will probably not work on your machine. I call MuseScore on my computer to open the midi file. Again, on your machine, go to the \"output/\" folder and open the midi file in any way that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.view()\n",
    "s2.view()\n",
    "s1_emb.view()\n",
    "s2_emb.view()\n",
    "s12_emb.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think it is easier to \"see\" the embedded algorithms or \"hear\" them? Sonification is a powerful tool because our ears can often hear patterns that are difficult for our eyes to pick out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
